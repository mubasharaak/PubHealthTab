{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a14697",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e24727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# import pickle5 as pickle\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, AutoModelForSequenceClassification\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification, RobertaConfig\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, AdamW, BertTokenizer, BertPreTrainedModel, BertModel,  RobertaForSequenceClassification, RobertaTokenizer\n",
    "from transformers import TapasForSequenceClassification, TapasTokenizer, TapasConfig \n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba37ea",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b68649",
   "metadata": {},
   "source": [
    "1. <strong>Set dataset files in dictionary 'dataset_dict' </strong>\n",
    "\n",
    "\n",
    "2. <strong>Evaluate hyperparamter selection using function main_evaluation. </strong>\n",
    "\n",
    "\n",
    "3. <strong>Predict using function main_prediction. </strong> \n",
    "\n",
    "Set the following variables first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f414b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to train, eval and test set files for each table FC dataset below\n",
    "\n",
    "# path to other datasets: \n",
    "# Infotabs: https://github.com/infotabs/infotabs\n",
    "# TabFact: https://github.com/wenhuchen/Table-Fact-Checking\n",
    "# FEVEROUS: https://github.com/Raldir/FEVEROUS\n",
    "\n",
    "dataset_dict = {\n",
    "    \"pubhealthtab_train\": \"\",\n",
    "    \"pubhealthtab_eval\": \"\",\n",
    "    \"pubhealthtab_test\": \"\",\n",
    "    \"feverous_train\": \"\",\n",
    "    \"feverous_eval\": \"\",\n",
    "    \"feverous_test\": \"\",\n",
    "    \"infotabs_train\": \"\",\n",
    "    \"infotabs_eval\": \"\",\n",
    "    \"infotabs_test\": \"\",\n",
    "    \"tabfact_train\": \"\",\n",
    "    \"tabfact_eval\": \"\",\n",
    "    \"tabfact_test\": \"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a5e89d",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0673386",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d953d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset \n",
    "\n",
    "class T5Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        try:\n",
    "            table = create_table_df(item[\"table\"]).astype(str) \n",
    "        except Exception as e: \n",
    "            print(f\"Error for index {idx}: {e}\")\n",
    "        \n",
    "        encoding = self.tokenizer(table=table,\n",
    "                                  queries=[item[\"claim\"]],\n",
    "                                  padding=\"max_length\",\n",
    "                                  truncation=True,\n",
    "                                  return_tensors=\"pt\"\n",
    "        )\n",
    "        # remove the batch dimension which the tokenizer adds by default\n",
    "        encoding = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        if item[\"label\"] == \"SUPPORTS\": \n",
    "            label = torch.tensor([1]) \n",
    "        elif item[\"label\"] == \"REFUTES\":\n",
    "            label = torch.tensor([2]) \n",
    "        else:\n",
    "            label = torch.tensor([0]) \n",
    "            \n",
    "        encoding['labels'] = label\n",
    "        return encoding\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a69a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset \n",
    "\n",
    "class TableDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        try:\n",
    "            table = create_table_df(item[\"table\"]).astype(str) \n",
    "        except Exception as e: \n",
    "            print(f\"Error for index {idx}: {e}\")\n",
    "        \n",
    "        encoding = self.tokenizer(table=table,\n",
    "                                  queries=[item[\"claim\"]],\n",
    "                                  padding=\"max_length\",\n",
    "                                  truncation=True,\n",
    "                                  return_tensors=\"pt\"\n",
    "        )\n",
    "        # remove the batch dimension which the tokenizer adds by default\n",
    "        encoding = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        # add the float_answer which is also required (weak supervision for aggregation case)\n",
    "        if item[\"label\"] == \"SUPPORTS\": \n",
    "            label = torch.tensor([1]) # 1 means entailed, 0 means refuted\n",
    "        elif item[\"label\"] == \"REFUTES\":\n",
    "            label = torch.tensor([2]) # 1 means entailed, 0 means refuted\n",
    "        else:\n",
    "            label = torch.tensor([0]) # 1 means entailed, 0 means refuted\n",
    "            \n",
    "        encoding['labels'] = label\n",
    "        return encoding\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0384e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"bert\": \"bert-base-uncased\",\n",
    "    \"roberta\": 'ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli',\n",
    "#     \"roberta\": 'roberta-base',\n",
    "    \"albert\": \"albert-base-v2\",\n",
    "    \"albert_pretrained_nli\": \"ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
    "    \"biobert\": \"gsarti/biobert-nli\",\n",
    "    \"bluebert\": \"adamlin/NCBI_BERT_pubmed_mimic_uncased_large_transformers\",\n",
    "    \"clinicalbert\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "    \"tapas\": \"google/tapas-base-finetuned-tabfact\"\n",
    "}\n",
    "\n",
    "model_hyperparameter_dict = {\n",
    "    \"bert\": {\n",
    "        \"training_epochs\": 5, \n",
    "        \"batch_size\": 4, \n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"weight_decay\": 0.001\n",
    "    },\n",
    "    \"roberta\": { \n",
    "        \"training_epochs\": 4, \n",
    "        \"batch_size\": 8, \n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "    \"albert\": {\n",
    "        \"training_epochs\": 5, \n",
    "        \"batch_size\": 16, \n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"weight_decay\": 0.001\n",
    "    },\n",
    "    \"albert_pretrained_nli\": {\n",
    "        \"training_epochs\": 5, \n",
    "        \"batch_size\": 8, \n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"weight_decay\": 0.001\n",
    "    },\n",
    "    \"biobert\": {\n",
    "        \"training_epochs\": 5, \n",
    "        \"batch_size\": 4, \n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"weight_decay\": 0.001    \n",
    "    },\n",
    "    \"bluebert\": {\n",
    "        \"training_epochs\": 5, \n",
    "        \"batch_size\": 8, \n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"weight_decay\": 0.001\n",
    "    },\n",
    "    \"clinicalbert\": {\n",
    "        \"training_epochs\": 4, \n",
    "        \"batch_size\": 4, \n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"weight_decay\": 0.01\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_dataset(dataset_name: str): \n",
    "    trainset_path = dataset_dict[dataset_name+\"_train\"]\n",
    "    testset_path = dataset_dict[dataset_name+\"_test\"]\n",
    "    evalset_path = dataset_dict[dataset_name+\"_eval\"]\n",
    "    \n",
    "    trainset = []\n",
    "    testset = []\n",
    "    evalset = []\n",
    "\n",
    "    if dataset_name == \"pubhealthtab\":\n",
    "        with jsonlines.open(trainset_path) as reader:\n",
    "            for line in reader: \n",
    "                trainset.append(line)\n",
    "\n",
    "        with jsonlines.open(testset_path) as reader:\n",
    "            for line in reader: \n",
    "                testset.append(line)\n",
    "\n",
    "        with jsonlines.open(evalset_path) as reader:\n",
    "            for line in reader: \n",
    "                evalset.append(line)\n",
    "\n",
    "    elif dataset_name == \"feverous\": \n",
    "        with open(trainset_path, \"rb\") as file:\n",
    "            trainset = pickle.load(file)\n",
    "\n",
    "        with open(testset_path, \"rb\") as file:\n",
    "            testset = pickle.load(file)\n",
    "\n",
    "        with open(evalset_path, \"rb\") as file:\n",
    "            evalset = pickle.load(file)\n",
    "    \n",
    "    else: \n",
    "        with open(trainset_path) as file:\n",
    "            trainset = json.load(file)\n",
    "\n",
    "        with open(testset_path) as file:\n",
    "            testset = json.load(file)\n",
    "\n",
    "        with open(evalset_path) as file:\n",
    "            evalset = json.load(file)\n",
    "\n",
    "    print(f\"Dataset split into {len(trainset)} training samples, {len(evalset)} eval samples and {len(testset)} test samples.\")\n",
    "    return trainset, evalset, testset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aabd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_caption(table: dict) -> str:\n",
    "    \"\"\"\n",
    "    Extracts caption text from table if one exists\n",
    "\n",
    "    Parameters:\n",
    "    table (dict): one table consisting of caption, header and rows   \n",
    "    \"\"\"\n",
    "\n",
    "    return (table[\"caption\"] if table[\"caption\"] else \"\") \n",
    "\n",
    "\n",
    "def get_table_header(table: dict) -> str:\n",
    "    \"\"\"\n",
    "    Extracts header text from table if one exists\n",
    "\n",
    "    Parameters:\n",
    "    table (dict): one table consisting of caption, header and rows   \n",
    "    \"\"\"\n",
    "    header = (\" \".join(table[\"header_horizontal\"]) if table[\"header_horizontal\"] and any(table[\"header_horizontal\"]) else \"\")\n",
    "    header = header + \" \" + (\" \".join(table[\"header_vertical\"]) if table[\"header_vertical\"] else \"\")\n",
    "\n",
    "    return header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f41e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_text(claim: str, table: dict, method: str, shuffle: bool, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from table returns table in the desired representation \n",
    "\n",
    "    Parameters:\n",
    "    table (dict): one table consisting of caption, header and rows   \n",
    "\n",
    "    Returns: \n",
    "    str: text representation of converted table \n",
    "\n",
    "    \"\"\"\n",
    "    random.seed(4)\n",
    "    \n",
    "    text = \"\"\n",
    "    caption = get_table_caption(table) \n",
    "    header = get_table_header(table)\n",
    "    rows = table[\"rows\"].copy()\n",
    "    shuffle_type = \"row\"\n",
    "    \n",
    "    if shuffle: \n",
    "        if shuffle_type == \"row\": \n",
    "            random.shuffle(rows)\n",
    "        else: \n",
    "            new_rows = []\n",
    "            for row in rows: \n",
    "                random.shuffle(row)\n",
    "                new_rows.append(row)\n",
    "            rows = new_rows.copy()\n",
    "    \n",
    "    if method == \"concatenation\":\n",
    "        content = \"\"\n",
    "        for row in rows:\n",
    "            row_content = \" \".join([str(entry) for entry in row])\n",
    "            content = content + row_content + \" \"\n",
    "\n",
    "        text = \" \".join([caption, header, content.strip()])     \n",
    "        \n",
    "    elif method == \"template\":\n",
    "        content = \"\"\n",
    "        for i_row, row in enumerate(rows):               \n",
    "            sent = f\"In row {str(i_row)} \"\n",
    "            for i_col, col in enumerate(row):\n",
    "                if table[\"header_horizontal\"] and len(table[\"header_horizontal\"])==len(row):\n",
    "                    sent += f\"column {str(i_col)} ({table['header_horizontal'][i_col]}) is {col}, \"\n",
    "                else:\n",
    "                    sent += f\"column {str(i_col)} is {col}, \"\n",
    "            sent += \". \"\n",
    "            content += sent\n",
    "        text = \" \".join([caption, header, content])\n",
    "    \n",
    "    elif method == \"t5_concat\":\n",
    "        content = \"\"\n",
    "        for row in rows:\n",
    "            row_content = \" \".join([entry for entry in row])\n",
    "            row_content_tokenized = tokenizer(row_content, truncation = True, max_length=len(row_content)+5, return_tensors=\"pt\").input_ids\n",
    "            generated_ids  = model.generate(row_content_tokenized, max_length=len(row_content)+5)\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "\n",
    "            row_content_t5 = preds[0]\n",
    "            content = content + row_content_t5 + \" \"\n",
    "\n",
    "        text = \" \".join([caption, header, content.strip()])\n",
    "\n",
    "    elif method == \"t5_temp\":\n",
    "        content = \"\"\n",
    "        for i_row, row in enumerate(rows):               \n",
    "            sent = f\"In row {str(i_row)} \"\n",
    "            for i_col, col in enumerate(row):\n",
    "                if table[\"header_horizontal\"] and len(table[\"header_horizontal\"])==len(row):\n",
    "                    sent += f\"column {str(i_col)} ({table['header_horizontal'][i_col]}) is {col}, \"\n",
    "                else:\n",
    "                    sent += f\"column {str(i_col)} is {col}, \"\n",
    "            sent += \". \"\n",
    "            sent_tokenized = tokenizer(sent, truncation = True, max_length = 512, return_tensors=\"pt\").input_ids\n",
    "            generated_ids  = model.generate(sent_tokenized)\n",
    "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "\n",
    "            sent_t5 = preds[0]\n",
    "            content += sent_t5\n",
    "        text = \" \".join([caption, header, content])\n",
    "\n",
    "    elif method == \"key_val\":\n",
    "        content = \"\"\n",
    "        for i_row, row in enumerate(rows):               \n",
    "            sent = f\"row_{str(i_row)}: \"\n",
    "            for i_col, col in enumerate(row):\n",
    "                if table[\"header_horizontal\"] and len(table[\"header_horizontal\"])==len(row):\n",
    "                    sent += f\"{table['header_horizontal'][i_col]}:{col}, \"\n",
    "                else:\n",
    "                    sent += f\"column_{str(i_col)}:{col}, \"\n",
    "            sent += \". \"\n",
    "            content += sent\n",
    "        text = \" \".join([caption, header, content])\n",
    "        \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(claim_verdict_list):\n",
    "    map_verdict_to_index = {'NOT ENOUGH INFO': 0, 'SUPPORTS': 1, 'REFUTES': 2}\n",
    "    text = [x[0] for x in claim_verdict_list] \n",
    "    labels = [map_verdict_to_index[x[1]] for x in claim_verdict_list] #get value from enum\n",
    "\n",
    "    return text, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e44511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_df(table_dict): \n",
    "    data = table_dict[\"rows\"].copy()\n",
    "    column_names = table_dict[\"header_horizontal\"].copy()\n",
    "    df_width = max(len(max(data,key=len)), len(column_names))\n",
    "\n",
    "    while len(column_names) < df_width: \n",
    "        column_names.insert(0,\"\")\n",
    "        \n",
    "    for row in data: \n",
    "        while len(row) < df_width: \n",
    "            row.insert(0,\"\")\n",
    "\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):    \n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    class_rep = classification_report(labels, preds, target_names= ['NOT ENOUGH INFO', 'SUPPORTS', 'REFUTES'], output_dict=True)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "    pd.DataFrame(conf_matrix).to_csv(\"confusion_matrix.csv\")\n",
    "    \n",
    "    result_matrix = pd.DataFrame(columns = [\"claim\", \"table\", \"label\", \"prediction\"])\n",
    "    for i, entry in enumerate(testset): \n",
    "        if entry[\"label\"] == \"SUPPORTS\": \n",
    "                label = 1 # 1 means entailed, 0 means refuted\n",
    "        elif entry[\"label\"] == \"REFUTES\":\n",
    "            label = 2 # 1 means entailed, 0 means refuted\n",
    "        else:\n",
    "            label = 0 # 1 means entailed, 0 means refuted\n",
    "\n",
    "        result_matrix = result_matrix.append({\"claim\": entry[\"claim\"], \"table\": entry[\"table\"], \n",
    "                                              \"label\": label, \"prediction\": preds[i]}, ignore_index=True)\n",
    "\n",
    "    pd.DataFrame(result_matrix).to_csv(\"result_matrix.csv\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_micro': f1,\n",
    "        'precision_micro': precision,\n",
    "        'recall_micro': recall,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'class_rep': class_rep\n",
    "    }\n",
    "\n",
    "def model_init():\n",
    "    return RobertaForSequenceClassification.from_pretrained(model_dict[\"roberta\"], num_labels=3, return_dict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692bc525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_trainer(model, train_dataset, training_epochs, batch_size, learning_rate, weight_decay):\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "    output_dir='./results',                  \n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    weight_decay=weight_decay,               \n",
    "    num_train_epochs=training_epochs,        \n",
    "    save_strategy=\"no\", \n",
    "    learning_rate = learning_rate,           \n",
    "    per_device_eval_batch_size=batch_size,   \n",
    "    disable_tqdm=True\n",
    "    )\n",
    "    \n",
    "    evaluation_args = TrainingArguments(\n",
    "        \"test\", evaluation_strategy=\"steps\", eval_steps=10000)\n",
    "\n",
    "    trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    compute_metrics = compute_metrics\n",
    "    )\n",
    "    \n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c9416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(annotation, t5_tokenizer, t5_model, representation: str, shuffle: str, model: str):\n",
    "    if shuffle == \"claim\": \n",
    "        claim = annotation[\"claim\"].split(\" \")\n",
    "        random.shuffle(claim)\n",
    "        claim_shuffled = \" \".join(claim)\n",
    "        sequence = [claim_shuffled]    \n",
    "        sequence.append(get_table_text(annotation[\"claim\"], annotation[\"table\"], method = representation, shuffle=False,\n",
    "                                       model=t5_model, tokenizer=t5_tokenizer))\n",
    "\n",
    "    elif shuffle == \"table\": \n",
    "        sequence = [annotation[\"claim\"]]    \n",
    "        sequence.append(get_table_text(annotation[\"claim\"], annotation[\"table\"], method = representation, shuffle=True,\n",
    "                                       model=t5_model, tokenizer=t5_tokenizer))\n",
    "    else:\n",
    "        sequence = [annotation[\"claim\"]]    \n",
    "        sequence.append(get_table_text(annotation[\"claim\"], annotation[\"table\"], method = representation, shuffle=False,\n",
    "                                       model=t5_model, tokenizer=t5_tokenizer))\n",
    "        \n",
    "    if model == \"roberta\":\n",
    "        return ' </s> '.join(sequence)\n",
    "    \n",
    "    else: \n",
    "        return ' [SEP] '.join(sequence)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_prediction(t5_model, t5_tokenizer, dataset: str, representation: str, model: str, \n",
    "                    df: pd.DataFrame, shuffle = \"\"): \n",
    "\n",
    "    # load dataset \n",
    "    trainset, evalset, testset = load_dataset(dataset) \n",
    "    \n",
    "    if model: \n",
    "        models_list = [model]\n",
    "    else: # execute predictions for multiple models \n",
    "        models_list = list(model_dict.keys())\n",
    "\n",
    "    for model_name in models_list: \n",
    "        model_path = model_dict[model_name]\n",
    "\n",
    "        # load model and tokenizer \n",
    "        if model_name == \"roberta\": \n",
    "            tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "            model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=3, return_dict=True)\n",
    "            \n",
    "        elif model_name == \"albert\" or model_name == \"albert_pretrained_nli\":\n",
    "            tokenizer = AlbertTokenizer.from_pretrained(model_path)\n",
    "            model = AlbertForSequenceClassification.from_pretrained(model_path, num_labels=3, return_dict=True)\n",
    "        elif model_name == \"tapas\":\n",
    "            tokenizer = TapasTokenizer.from_pretrained(model_path)\n",
    "            config = TapasConfig.from_pretrained(model_path)\n",
    "            config.num_labels = 3\n",
    "            model = TapasForSequenceClassification(config)\n",
    "        else: \n",
    "            tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "            model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3, return_dict=True)\n",
    "            \n",
    "        # preparing data for training\n",
    "        # convert tables into desired representation \n",
    "        if model_name != \"tapas\":\n",
    "            print(\"start preparing trainset.\")\n",
    "            input_train = [(prepare_input(entry, t5_tokenizer, t5_model, representation, shuffle, model), entry[\"label\"], model) for entry in trainset]\n",
    "            \n",
    "            print(\"start preparing testset.\")\n",
    "            input_test = [(prepare_input(entry, t5_tokenizer, t5_model, representation, shuffle, model), entry[\"label\"], model) for entry in testset]\n",
    "\n",
    "            text_train, labels_train = process_data(input_train)\n",
    "            text_test, labels_test = process_data(input_test)\n",
    "            \n",
    "            text_train_tok = tokenizer(text_train, padding=True, truncation=True, max_length=512)\n",
    "            train_dataset = MYDataset(text_train_tok, labels_train)\n",
    "\n",
    "            text_test_tok = tokenizer(text_test, padding=True, truncation=True, max_length=512)\n",
    "            test_dataset = MYDataset(text_test_tok, labels_test)\n",
    "            \n",
    "        else:\n",
    "            train_dataset = TableDataset(trainset, tokenizer)\n",
    "            test_dataset = TableDataset(testset, tokenizer)\n",
    "\n",
    "        e = model_hyperparameter_dict[model_name][\"training_epochs\"]\n",
    "        b = model_hyperparameter_dict[model_name][\"batch_size\"]\n",
    "        l = model_hyperparameter_dict[model_name][\"learning_rate\"]\n",
    "        w = model_hyperparameter_dict[model_name][\"weight_decay\"]\n",
    "        print(f\"Running training with following parameters: model={model_name}, representation={representation}, epoch={e}, batch size={b}, learning rate={l}, weight decay={w}.\")\n",
    "\n",
    "        # train model \n",
    "        trainer = model_trainer(model, train_dataset, training_epochs=e, batch_size=b, \n",
    "                                learning_rate=l, weight_decay=w)\n",
    "        trainer.train() # training \n",
    "\n",
    "        # test\n",
    "        predictions = trainer.predict(test_dataset) # testing\n",
    "    \n",
    "        print(f\"Prediction is: {predictions.metrics['test_f1_macro']}.\")\n",
    "\n",
    "        df.loc[len(df)] = [dataset, model_name, e, b, l, w, \n",
    "                           predictions.metrics['test_f1_micro'], \n",
    "                           predictions.metrics['test_f1_macro'], \n",
    "                           predictions.metrics['test_class_rep']['SUPPORTS']['f1-score'], \n",
    "                           predictions.metrics['test_class_rep']['REFUTES']['f1-score'], \n",
    "                           predictions.metrics['test_class_rep']['NOT ENOUGH INFO']['f1-score'], \n",
    "                           predictions.metrics['test_loss'],\n",
    "                           predictions.metrics['test_accuracy'], predictions.metrics]\n",
    "\n",
    "        print(f\"Saving results into file: fc_PREDICTION_{dataset}_{representation}_{model_name}.csv\")\n",
    "        df.to_csv(f'./fc_PREDICTION_{dataset}_{representation}_{model_name}.csv')\n",
    "\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d94eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_evaluation(t5_model, t5_tokenizer, dataset: str, representation: str, model: str, \n",
    "                    df: pd.DataFrame, shuffle = \"\"): \n",
    "\n",
    "    # load dataset \n",
    "    trainset, evalset, testset = load_dataset(dataset) \n",
    "    model_name = model\n",
    "\n",
    "    # convert tables into desired representation \n",
    "    print(\"Preparing data with desired representation.\")\n",
    "    \n",
    "    if model_name != \"tapas\":\n",
    "        input_train = [(prepare_input(entry, representation, shuffle, t5_tokenizer, t5_model, model), entry[\"label\"], model) for entry in trainset]\n",
    "        input_eval = [(prepare_input(entry, representation, shuffle, t5_tokenizer, t5_model, model), entry[\"label\"], model) for entry in evalset]\n",
    "        input_test = [(prepare_input(entry, representation, shuffle, t5_tokenizer, t5_model, model), entry[\"label\"], model) for entry in testset]\n",
    "\n",
    "        text_train, labels_train = process_data(input_train)\n",
    "        text_eval, labels_eval = process_data(input_eval)\n",
    "        text_test, labels_test = process_data(input_test)\n",
    "\n",
    "    if model: \n",
    "        models_list = [model]\n",
    "    else: \n",
    "        models_list = list(model_dict.keys())\n",
    "        \n",
    "    batch_sizes = [4, 8, 16, 32]\n",
    "    training_epochs = [10, 15, 20, 30]\n",
    "    learning_rates = [1e-3, 1e-5, 1e-7]\n",
    "    weight_decays = [0.01, 0.001, 0.0001]\n",
    "\n",
    "    for model_name in models_list: \n",
    "        model_path = model_dict[model_name]\n",
    "\n",
    "        if model_name == \"roberta\": \n",
    "            tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "        elif model_name == \"albert\" or model_name == \"albert_pretrained_nli\":\n",
    "            tokenizer = AlbertTokenizer.from_pretrained(model_path)\n",
    "        elif model_name == \"tapas\":\n",
    "            tokenizer = TapasTokenizer.from_pretrained(model_path, num_labels=3)\n",
    "        else: \n",
    "            tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        print(\"Preparing data for training.\")\n",
    "        \n",
    "        if model_name == \"tapas\":\n",
    "            train_dataset = TableDataset(trainset, tokenizer)\n",
    "            eval_dataset = TableDataset(evalset, tokenizer)\n",
    "            test_dataset = TableDataset(testset, tokenizer)\n",
    "        else:\n",
    "            text_train_tok = tokenizer(text_train, padding=True, truncation=True, max_length=512)\n",
    "            train_dataset = MYDataset(text_train_tok, labels_train)\n",
    "\n",
    "            text_eval_tok = tokenizer(text_eval, padding=True, truncation=True, max_length=512)\n",
    "            eval_dataset = MYDataset(text_eval_tok, labels_eval)\n",
    "\n",
    "            text_test_tok = tokenizer(text_test, padding=True, truncation=True, max_length=512)\n",
    "            test_dataset = MYDataset(text_test_tok, labels_test)\n",
    "\n",
    "        for e in training_epochs:\n",
    "            for b in batch_sizes: \n",
    "                for l in learning_rates: \n",
    "                    for w in weight_decays:\n",
    "                        print(f\"Running training with following parameters: representation={representation}, epoch={e}, batch size={b}, learning rate={l}, weight decay={w}.\")\n",
    "                        \n",
    "                        if model_name == \"roberta\": \n",
    "                            print(\"Loading RoBERTa model.\")\n",
    "                            model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=3, return_dict=True)\n",
    "                        elif model_name == \"albert\" or model_name == \"albert_pretrained_nli\":\n",
    "                            model = AlbertForSequenceClassification.from_pretrained(model_path, num_labels=3, return_dict=True)\n",
    "                        elif model_name == \"tapas\":\n",
    "                            config = TapasConfig.from_pretrained(model_path)\n",
    "                            config.num_labels = 3\n",
    "                            model = TapasForSequenceClassification(config)\n",
    "                        else: \n",
    "                            model = BertForSequenceClassification.from_pretrained(model_path, num_labels=3, return_dict=True)\n",
    "                        \n",
    "                        # train model \n",
    "                        trainer = model_trainer(model, train_dataset, training_epochs=e, batch_size=b, \n",
    "                                                learning_rate=l, weight_decay=w)\n",
    "                        trainer.train() # training \n",
    "                        \n",
    "                        # validate \n",
    "                        predictions = trainer.predict(eval_dataset) # testing\n",
    "\n",
    "                        # test\n",
    "#                         predictions = trainer.predict(test_dataset) # testing\n",
    "                        \n",
    "                        df.loc[len(df)] = [dataset, model_name, e, b, l, w, \n",
    "                                           predictions.metrics['test_f1_micro'], \n",
    "                                           predictions.metrics['test_f1_macro'], \n",
    "                                           predictions.metrics['test_class_rep']['SUPPORTS']['f1-score'], \n",
    "                                           predictions.metrics['test_class_rep']['REFUTES']['f1-score'], \n",
    "                                           predictions.metrics['test_class_rep']['NOT ENOUGH INFO']['f1-score'], \n",
    "                                           predictions.metrics['test_loss'],\n",
    "                                           predictions.metrics['test_accuracy'], predictions.metrics]\n",
    "                        \n",
    "                        print(f\"Saving results into file: fc_experiment_{dataset}_{representation}_{model_name}.csv\")\n",
    "                        df.to_csv(f'./fc_experiment_{dataset}_{representation}_{model_name}.csv')\n",
    "                        \n",
    "        return df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d861faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = True\n",
    "\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"Narrativa/t5-base-finetuned-totto-table-to-text\")\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"Narrativa/t5-base-finetuned-totto-table-to-text\")\n",
    "# load global variables \n",
    "print(\"Loading T5 model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8164876",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd03e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION FOR HYPERPARAMETER TUNING \n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    results_df = pd.DataFrame(columns=[\"dataset\", \"model\", \"epochs\", \"batch_size\", \"learning_rate\", \"weight_decay\", \n",
    "                                       \"f1_micro\", \"f1_macro\", \"f1_SUPPORTS\", \"f1_REFUTES\", \"f1_NEI\", \n",
    "                                       \"loss\", \"accuracy\", \"metrics\"])\n",
    "    data = \"pubhealthtab\" # set dataset name, e.g. infotabs, pubhealthtab, ... \n",
    "    rep = \"concatenation\" # set representation type\n",
    "    mod = \"roberta\" # set model name\n",
    "    \n",
    "    results_df_updated = main_evaluation(t5_model, t5_tokenizer, dataset=data, representation=rep, \n",
    "                                         model=mod, df=results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c07c34",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION \n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    results_df = pd.DataFrame(columns=[\"dataset\", \"model\", \"epochs\", \"batch_size\", \"learning_rate\", \"weight_decay\", \n",
    "                                       \"f1_micro\", \"f1_macro\", \"f1_SUPPORTS\", \"f1_REFUTES\", \"f1_NEI\",\n",
    "                                       \"loss\", \"accuracy\", \"metrics\"])\n",
    "    # predictions for all models\n",
    "    data = \"pubhealthtab\" # set dataset name, e.g. infotabs, pubhealthtab, ... \n",
    "    rep = \"concatenation\" # set representation type\n",
    "    mod = \"roberta\" # set model name\n",
    "    \n",
    "    results_df_updated = main_prediction(t5_model, t5_tokenizer, dataset=data, representation=rep, \n",
    "                                         model=mod, df=results_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c6e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
