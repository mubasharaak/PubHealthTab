{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78085ad7",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ff495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import bs4\n",
    "import elq.main_dense as main_dense\n",
    "import glob\n",
    "import io\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pickle5 as pickle # if required e.g. error with normal pickle package\n",
    "import pymongo\n",
    "import qwikidata\n",
    "\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import requests.exceptions\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import tempfile\n",
    "import tabula\n",
    "import tagme\n",
    "import tldextract\n",
    "import torch\n",
    "import transformers\n",
    "import utils \n",
    "import urllib.parse\n",
    "import uuid\n",
    "import unicodedata\n",
    "import wikipedia\n",
    "import zipfile\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter, deque, OrderedDict\n",
    "from concurrent.futures import *\n",
    "from datetime import date\n",
    "from langdetect import DetectorFactory, detect, detect_langs\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from requests_futures.sessions import *\n",
    "from requests.exceptions import ReadTimeout, TooManyRedirects, ConnectionError, ConnectTimeout,\\\n",
    "    InvalidSchema, InvalidURL\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from transformers import *\n",
    "from typing import Dict\n",
    "from urllib.parse import urlsplit\n",
    "from urllib.parse import urlparse\n",
    "from utils import *\n",
    "\n",
    "from operator import itemgetter\n",
    "from qwikidata.entity import WikidataItem, WikidataLexeme, WikidataProperty\n",
    "from qwikidata.linked_data_interface import get_entity_dict_from_api\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "\n",
    "#nltk.download('punkt')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78abeefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba37ea",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b68649",
   "metadata": {},
   "source": [
    "1. <strong>Link claims to Wikimedia entities </strong>\n",
    "\n",
    "\n",
    "2. <strong>Get referenced websites from Wikipedia </strong>\n",
    "\n",
    "\n",
    "3. <strong>Get top Tables from extracted Evidence Websites</strong> \n",
    "\n",
    "\n",
    "Set the following variables first:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb15a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to db config containing pre-processed data which should be used for evidence table extraction\n",
    "# Or load data from _trainset_evidence_extraction.pkl and _testset_evidence_extraction.pkl\n",
    "path_config_db = ''\n",
    "path_trainset = ''\n",
    "path_testset = ''\n",
    "\n",
    "train_data = pd.read_pickle(path_trainset)\n",
    "test_data = pd.read_pickle(path_testset)\n",
    "\n",
    "# Path to ELQ model used for Wikipedia entity linking \n",
    "path_elq_model = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d89544d",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092804b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB connection - skip if data directly loaded from .pkl files\n",
    "\n",
    "db_client = pymongo.MongoClient(path_config_db) \n",
    "db = db_client.pubhealth\n",
    "train_col = db.trainset\n",
    "test_col = db.testset\n",
    "\n",
    "read_train_set = True\n",
    "read_test_set = True\n",
    "\n",
    "if read_train_set:\n",
    "    train_data = pd.DataFrame(list(cursor)) \n",
    "    print(f\"Length of training set: {len(train_data)}\")\n",
    "\n",
    "if read_test_set:\n",
    "    test_data = pd.DataFrame(list(cursor)) \n",
    "    print(f\"Length of test set: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7593c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5901ff23",
   "metadata": {},
   "source": [
    "##### Load preprocessed claims and extracted tables in a new MongoDB collection for MTurk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99621df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserted some intermediate results back into DB \n",
    "\n",
    "if insert_entries: \n",
    "    for index, row in train_data.iterrows(): \n",
    "        print('Inserted into final_dataset', index, end=\"\\r\")\n",
    "        \n",
    "        final_col.update_one({'_id': row[\"_id\"]},\n",
    "                             {'$set': {'tables': table_result[index]}})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355aca87",
   "metadata": {},
   "source": [
    "### (1) <strong>Link claims to Wikimedia items</strong> \n",
    "\n",
    "a. Linking with WAT (successor of TagME)\n",
    "\n",
    "b. Linking with ELQ entity linker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eecb98",
   "metadata": {},
   "source": [
    "#### a.) Linking with WAT (successor of TagME)\n",
    "TagME cited as baseline in Wikidata/-pedia entity linking task papers e.g. 'Efficient One-Pass End-to-End Entity Linking for Questions' (Belinda et al., 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08315a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "    output = get_wikidata_items_from_txt_multiprocess(train_data, num_processes)\n",
    "    \n",
    "    wiki_entities_WAT = [[i[1] for i in entry] if any(entry) else [] for entry in output]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WATAnnotation:\n",
    "\n",
    "    def __init__(self, d):\n",
    "        self.start = d['start']\n",
    "        self.end = d['end']\n",
    "\n",
    "        # annotation accuracy\n",
    "        self.rho = d['rho']\n",
    "        self.prior_prob = d['explanation']['prior_explanation']['entity_mention_probability']\n",
    "\n",
    "        # annotated text\n",
    "        self.spot = d['spot']\n",
    "\n",
    "        # Wikpedia entity info\n",
    "        self.wiki_id = d['id']\n",
    "        self.wiki_title = d['title']\n",
    "\n",
    "\n",
    "    def json_dict(self):\n",
    "        return {'wiki_title': self.wiki_title,\n",
    "                'wiki_id': self.wiki_id,\n",
    "                'start': self.start,\n",
    "                'end': self.end,\n",
    "                'rho': self.rho,\n",
    "                'prior_prob': self.prior_prob\n",
    "                }\n",
    "\n",
    "\n",
    "def get_wikidata_qid(wiki_title: str) -> str: \n",
    "    \"\"\"\n",
    "    Returns for a Wikipedia title the corresponding Wikidata item\n",
    "    \n",
    "    Parameters: \n",
    "    wiki_title (str): Wikipedia page title for which corresponding item is searched\n",
    "    \n",
    "    Returns:\n",
    "    str: QID of retrieved item\n",
    "    \"\"\"\n",
    "    qid = None\n",
    "    \n",
    "    wikipedia_url = \"https://en.wikipedia.org/w/api.php\"    \n",
    "    session = requests.Session()\n",
    "    params = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"prop\": \"pageprops\",\n",
    "    \"titles\": wiki_title\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = session.get(url=wikipedia_url, params=params)\n",
    "        response_data = response.json()\n",
    "\n",
    "        if response_data and 'query' in response_data and 'pages' in response_data['query']:\n",
    "            for pages in response.json()['query']['pages'].values():\n",
    "                if 'pageprops' in pages and 'wikibase_item' in pages['pageprops']:\n",
    "                    qid = pages['pageprops']['wikibase_item']\n",
    "\n",
    "    except Exception as e: \n",
    "        print(f\"Following error occured while execution of function get_wikidata_qid: {e}.\")\n",
    "        print(f\"Value of response object: {response}.\")\n",
    "        \n",
    "    return qid\n",
    "\n",
    "\n",
    "def get_wikidata_item(qid: str) -> WikidataItem: \n",
    "    \"\"\"\n",
    "    Returns Wikidata item given its 'qid'\n",
    "    \n",
    "    Parameters: \n",
    "    qid (str): qid of Wikidata item which is searched\n",
    "    \n",
    "    Returns: \n",
    "    WikidataItem: Wikidata item object for given qid\n",
    "    \"\"\"\n",
    "    try:\n",
    "#         q_dict = get_entity_dict_from_api(qid)\n",
    "        qitem = None\n",
    "        if qid:\n",
    "            q_dict = get_entity_dict_from_api(qid)\n",
    "            qitem = WikidataItem(q_dict)\n",
    "    \n",
    "    except Exception as e: \n",
    "        print(f\"Following exception occurred while retrieving WikidataItem: {e}.\")\n",
    "        \n",
    "    return qitem\n",
    "\n",
    "    \n",
    "def get_wikidata_items_from_txt(input_data) -> list: \n",
    "    \"\"\"\n",
    "    Return for a given text all linked Wikidata items\n",
    "    \n",
    "    Parameters: \n",
    "    input_data: object consisting of a string (= text for entity linking) and an int (= index)\n",
    "\n",
    "    Returns: \n",
    "    list: list of Q_IDs of linked items\n",
    "    \"\"\"\n",
    "    text, i = input_data\n",
    "    return_qid = False\n",
    "    rho_threshold = 0.3\n",
    "    \n",
    "    print(f\"Index of entry being printed: {i}.\")\n",
    "    \n",
    "    wat_url = 'https://wat.d4science.org/wat/tag/tag'\n",
    "    tagme.GCUBE_TOKEN = \"...\" # todo enter personal tagme token    \n",
    "    \n",
    "    payload = [(\"gcube-token\", tagme.GCUBE_TOKEN),\n",
    "               (\"text\", text),\n",
    "               (\"lang\", 'en'),\n",
    "               (\"tokenizer\", \"nlp4j\"),\n",
    "               ('debug', 9),\n",
    "               (\"method\",\n",
    "                \"spotter:includeUserHint=true:includeNamedEntity=true:includeNounPhrase=true,prior:k=50,filter-valid,centroid:rescore=true,topk:k=5,voting:relatedness=lm,ranker:model=0046.model,confidence:model=pruner-wiki.linear\")]\n",
    "\n",
    "    response = requests.get(wat_url, params=payload)\n",
    "    wikidata_entities = []\n",
    "\n",
    "    try:\n",
    "        response_data = response.json()\n",
    "    except Exception as e: \n",
    "        print(f\"Expection occurred while calling json() on response: {response}\")\n",
    "    \n",
    "#     if return_qid:\n",
    "#         wikidata_entities = [get_wikidata_qid(a['title']) for a in response_data['annotations'] if a and a['rho']>rho_threshold]\n",
    "#     else:\n",
    "#         wikidata_entities = [a['title'] for a in response.json()['annotations'] if a and a['rho']>rho_threshold]\n",
    "    \n",
    "    wikidata_entities = [(a['spot'], a['title'], get_wikidata_qid(a['title']), a['rho']) for a in response_data['annotations'] if a] #  and a['rho']>rho_threshold\n",
    "    \n",
    "    return wikidata_entities\n",
    "\n",
    "\n",
    "def get_wikidata_items_from_txt_multiprocess(df: pd.DataFrame, num_processes: int):\n",
    "    pool = multiprocessing.Pool(processes = num_processes)\n",
    "    \n",
    "    result = pool.map(get_wikidata_items_from_txt, zip(df[\"claim\"], df.index.values))\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f65cc5",
   "metadata": {},
   "source": [
    "#### b.) Linking with ELQ entity linker\n",
    "Switch to conda environment blink37 provided by authors of ELQ entity linker; Already done for entire training and test set\n",
    "\n",
    "Paper for ELQ linker: https://arxiv.org/pdf/2010.02413.pdf\n",
    "\n",
    "Github repo: https://github.com/facebookresearch/BLINK/tree/master/elq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d736d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ELQ model \n",
    "\n",
    "models_path = path_elq_model # todo the path where you stored the BLINK models\n",
    "config = {\n",
    "    \"interactive\": False,\n",
    "    \"biencoder_model\": models_path+\"elq_wiki_large.bin\",\n",
    "    \"biencoder_config\": models_path+\"elq_large_params.txt\",\n",
    "    \"cand_token_ids_path\": models_path+\"entity_token_ids_128.t7\",\n",
    "    \"entity_catalogue\": models_path+\"entity.jsonl\",\n",
    "    \"entity_encoding\": models_path+\"all_entities_large.t7\",\n",
    "    \"output_path\": \"logs/\", # logging directory\n",
    "    \"faiss_index\": \"none\",\n",
    "    \"index_path\": models_path+\"faiss_hnsw_index.pkl\",\n",
    "    \"num_cand_mentions\": 10,\n",
    "    \"num_cand_entities\": 10,\n",
    "    \"threshold_type\": \"joint\",\n",
    "    \"threshold\": -4.5,\n",
    "}\n",
    "args = argparse.Namespace(**config)\n",
    "models = main_dense.load_models(args, logger=logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d6d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut claim length to 128 if longer than that\n",
    "count = 0\n",
    "for index, row in df.iterrows(): \n",
    "    if len(row[\"claim\"])>128:\n",
    "        count += 1\n",
    "        df.at[index,\"claim\"] = row[\"claim\"][:128]\n",
    "\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628bae5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### IF HUGE DATASET AND MULTIPROCESSING CHOOSE CODE BELOW\n",
    "\n",
    "# Convert claims similarly to data_to_link dict object\n",
    "prediction_input = df[[\"claim_id\", \"claim\"]]\n",
    "prediction_input[\"claim\"] = prediction_input[\"claim\"].str.lower()\n",
    "prediction_input = prediction_input.rename(columns = {'claim_id': 'id', 'claim': 'text'})\n",
    "\n",
    "data_to_link = prediction_input.to_dict('records')\n",
    "\n",
    "predictions = predict_multiprocessing(data_to_link)\n",
    "predictions[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa203537",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### OTHERWISE THIS:\n",
    "\n",
    "# Convert claims similarly to data_to_link dict object\n",
    "prediction_input = df[[\"claim_id\", \"claim\"]]\n",
    "prediction_input[\"claim\"] = prediction_input[\"claim\"].str.lower()\n",
    "prediction_input = prediction_input.rename(columns = {'claim_id': 'id', 'claim': 'text'})\n",
    "\n",
    "data_to_link = prediction_input.to_dict('records')\n",
    "\n",
    "predictions = main_dense.run(args, None, *models, test_data=data_to_link)\n",
    "\n",
    "# Add predicted Wikipedia titles as column to dataframe \n",
    "df[\"wiki_entities_eql\"] = \"\"\n",
    "count = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    entities_linking = []\n",
    "    for linking in predictions[count]['pred_tuples_string']:\n",
    "        entities_linking.append(linking[0])\n",
    "        \n",
    "    count += 1\n",
    "    df.at[index,\"wiki_entities_eql\"] = entities_linking\n",
    "    \n",
    "# Compare claims, wikipedia titles old and wikipedia titles new to each other\n",
    "entity_df = pd.DataFrame({'claim': df[\"claim\"], \n",
    "                          'entities_eql': df[\"wiki_entities_eql\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5919589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all mini_train_data dataframes to each other \n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed0e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_parallel(data_to_link):\n",
    "    predictions = main_dense.run(args, None, *models, test_data=[data_to_link])\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def predict_multiprocessing(data_to_link: list):\n",
    "    \n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "    print(f\"Number of processes: {num_processes}\")\n",
    "\n",
    "    pool = multiprocessing.Pool(processes=num_processes)\n",
    "    output = pool.map(predict_parallel, data_to_link)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b370e8eb",
   "metadata": {},
   "source": [
    "#### Save Wiki entities to MongoDB OR .pkl file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f27da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save mini_train_data with both entities\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    test_col.update_one({'_id': row[\"_id\"]},\n",
    "                        {'$set': {'wiki_entities_eql': row['wiki_entities_eql']}})\n",
    "\n",
    "# path_intermediate_results = \"\" # set path to save intermediate results \n",
    "# df.to_pickle(path_intermediate_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901bbabb",
   "metadata": {},
   "source": [
    "### (2) Get Wikipedia articles (for entities linked with EQL & WAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6100e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "    result = multiprocessing_sort_wikipediapages(test_data, num_processes, col_eql = 'wiki_entities_eql', \n",
    "                                                 col_WAT = 'wiki_entities_WAT')\n",
    "    test_data[\"wiki_entities\"] = [entry if entry and entry!=[] else [] for entry in result]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd14163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update MongoDB \n",
    "for index, row in test_data.iterrows():\n",
    "    test_col.update_one({'_id': row[\"_id\"]},\n",
    "                        {'$set': {'wiki_entities': result[index]}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f473de6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Interface lemma tokenizer from nltk with sklearn\n",
    "class LemmaTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "\n",
    "# Lemmatize the stop words\n",
    "stop_words = set(stopwords.words('english')) \n",
    "tokenizer = LemmaTokenizer()\n",
    "token_stop = tokenizer(' '.join(stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785997f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_rank(claim, docs, top_k = 20): \n",
    "    \"\"\"\n",
    "    Retrieves and ranks documents \n",
    "    \"\"\"\n",
    "    top_docs = get_top_docs_bm25(claim, docs, top_k)\n",
    "    assert isinstance(top_docs, list)\n",
    "    \n",
    "#     reranked_docs = rerank_crossencoder(model, claim, top_docs)\n",
    "#     assert isinstance(reranked_docs, list)\n",
    "\n",
    "    return top_docs[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5bf516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_docs_bm25(claim: str, docs: list, top_k = 20): \n",
    "    logger.info(\"Executing function 'get_top_docs_bm25'...\")\n",
    "    \n",
    "    tokenized_docs = [doc.split(\" \") for doc in docs]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "    tokenized_query = claim.split(\" \")\n",
    "    top_doc = bm25.get_top_n(tokenized_query, docs, n=top_k)\n",
    "    \n",
    "    return top_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_crossencoder(model, claim, top_docs): \n",
    "    \"\"\"\n",
    "    Rerank retrieved top_docs return dict with documents and socres\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing function 'rerank_crossencoder'...\")\n",
    "    \n",
    "    sentences = list(zip([claim]*len(top_docs), top_docs))\n",
    "    scores = model.predict(sentences)\n",
    "    \n",
    "    doc_dict = dict(zip(top_docs, scores))\n",
    "    top_reranked_docs = dict(sorted(doc_dict.items(), key = itemgetter(1), reverse=True))\n",
    "\n",
    "    return list(top_reranked_docs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiprocessing_sort_wikipediapages(df: pd.DataFrame, num_processes: int, col_eql: str, col_WAT: str):\n",
    "    \"\"\"\n",
    "    Calls function 'sort_wikipediapages' using multiple threads \n",
    "    \"\"\"\n",
    "    pool = multiprocessing.Pool(processes = num_processes)\n",
    "    result = pool.map(sort_wikipediapages, zip(df[\"claim\"], df[col_eql], df[col_WAT], df.index.values))\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baef010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_wikipediapages(input_data) -> list: \n",
    "    \"\"\"\n",
    "    Given the claim's text and linked entites/Wikipedia pages, \n",
    "    returns an ordered list of Wikipedia pages based on similarity with claim's text\n",
    "    \n",
    "    Parameters:\n",
    "    input_data: consists of claim, entities_eql, index in the given order\n",
    "    \n",
    "    Return: \n",
    "    list: ranked list where the first Wikipedia page is the most similiar to the claim\n",
    "    \"\"\"    \n",
    "    claim, entities_eql, entities_WAT, index = input_data\n",
    "    top_k = 10\n",
    "    \n",
    "    logger.info(f\"Row with index {index} being processed...\")\n",
    "\n",
    "    assert isinstance(claim, str)\n",
    "    assert isinstance(entities_eql, list)\n",
    "    assert isinstance(entities_WAT, list)\n",
    "    \n",
    "    try:\n",
    "        wiki_pages = wikipedia.search(claim, results=10)\n",
    "    except Exception as e:\n",
    "        wiki_pages = []\n",
    "    \n",
    "    # merge entities (retrieved differently) into one list\n",
    "    pages = []\n",
    "    pages.extend(entities_eql)\n",
    "    pages.extend(entities_WAT)\n",
    "    pages.extend(wiki_pages)\n",
    "    pages = list(set(pages)) # delete duplicates in list\n",
    "    \n",
    "    # retrieve pages' text and save with title in dict \n",
    "    wikipages_dicts = {}\n",
    "    for page_title in pages: \n",
    "        try:\n",
    "            page = wikipedia.page(page_title)\n",
    "            wikipages_dicts[page.content] = page.title\n",
    "        except Exception as e: \n",
    "            continue\n",
    "\n",
    "    print(\"Start retrieval and ranking...\")\n",
    "    if wikipages_dicts != {}:\n",
    "        docs_sorted = retrieve_and_rank(claim, list(wikipages_dicts.keys()), top_k)    \n",
    "        result_list = [wikipages_dicts[doc] for doc in docs_sorted][:top_k]\n",
    "    else:\n",
    "        result_list = []\n",
    "    \n",
    "    print(f\"lengths of result list is: {len(result_list)}\")\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30323aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of entities \n",
    "entries_len = [len(entries) for entries in train_data[\"wiki_entities\"] if type(entries)==list]\n",
    "\n",
    "ax = sns.boxplot(x=entries_len)\n",
    "ax.set_title(\"Number of Wiki entities linked for claims\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4671ba3",
   "metadata": {},
   "source": [
    "### (3) <strong>Get referenced websites from Wikipedia and rank them</strong> \n",
    "\n",
    "a. get websites cited in Wikipedia articles (similarity to claim) \n",
    "\n",
    "b. sort all websites based on similarity to claim \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206b244",
   "metadata": {},
   "source": [
    "#### 1.) Extract referenced websites from Wikipedia articles  (from sorted_wikipedia_articles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9aeec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get references from top_j Wikipedia articles => todo: set top_j in function 'get_wikipedia_externallinks'\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "    output = multiprocessing_get_wikipedia_externallinks(train_data, num_processes)\n",
    "#     train_data[\"urls_wikipedia\"] = [entry if any(entry) else [] for entry in output]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61113208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of referenced Websites \n",
    "entries_len = [len(entries) for entries in output]\n",
    "print(pd.DataFrame(entries_len).describe())\n",
    "\n",
    "ax = sns.boxplot(x=entries_len)\n",
    "ax.set_title(\"Number of references extracted from corresp. Wikipedia articles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd8ba04",
   "metadata": {},
   "source": [
    "#### 2.) Sort all websites based on similarity to claim\n",
    "\n",
    "* return top-k (k=50) websites from all references\n",
    "* two options tried out: a) (2,3) tfidf b) bm25 => decided to go for bm25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4aab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): \n",
    "    return get_top_wikimedia_references_multiprocessing(train_data) # using column \"urls_wikipedia\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683762c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    result = main()\n",
    "    test_data[\"sorted_wiki_references\"] = result    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of entities \n",
    "entries_len = [len(entries) for entries in train_data[\"sorted_wiki_references\"] if entries and type(entries)==list]\n",
    "df = pd.DataFrame(entries_len)\n",
    "\n",
    "print(df.describe())\n",
    "\n",
    "ax = sns.boxplot(x=entries_len)\n",
    "ax.set_title(\"Number of tables extracted for each claim\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54f23b",
   "metadata": {},
   "source": [
    "#### Depricated: Extract references from Wikidata entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c04482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikidata entities have been retrieved before and ranked saved in column \"wiki_entities_sorted\" and \"wiki_entities_WAT\"\n",
    "# ELQ entities\n",
    "use_wikidata_tables = False\n",
    "\n",
    "if __name__ == \"__main__\" and use_wikidata_tables: \n",
    "    output = get_reference_items_title_given_multiprocess(df, wiki_entities = \"wiki_entities\")\n",
    "    df[\"sources_wikidata\"] = [entry if any(entry) else [] for entry in output]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00acfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_wikidata_tables:\n",
    "    wikidata_sources = [[list(entry.values())[0][0] for entry in source_list if list(entry.values())[0]] for source_list in df[\"sources_wikidata\"]]\n",
    "    df[\"urls_wikidata\"] = wikidata_sources.copy()\n",
    "    \n",
    "    # Update MongoDB with wikidata URLs\n",
    "    for index, row in df.iterrows():\n",
    "        test_col.update_one({'_id': row[\"_id\"]},\n",
    "                            {'$set': {'urls_wikidata': row[\"urls_wikidata\"]}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9152451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_items_title_given_multiprocess(df: pd.DataFrame, wiki_entities: str): \n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(processes = (num_processes//2))\n",
    "    \n",
    "    result = pool.map(get_reference_items_titles_given, zip(df[wiki_entities], df.index.values)) # TODO set column with entities\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_reference_items_titles_given(input_data) -> list: \n",
    "    \"\"\"\n",
    "    Entity matching has been previously done, therefore only extraction of resources necessary\n",
    "    \"\"\"\n",
    "    return_urls = True \n",
    "    wiki_titles, claim, index = input_data\n",
    "    \n",
    "    logger.info(f\"Function get_reference_items_titles_given being executed for index: {index}.\")\n",
    "    ref_items = []\n",
    "    \n",
    "    try:\n",
    "        # extract Wikidata items for linked wiki entity\n",
    "        q_items = [get_wikidata_item(get_wikidata_qid(t)) for t in wiki_titles if t]\n",
    "        \n",
    "        for item in q_items: \n",
    "            if not item:\n",
    "                continue\n",
    "            \n",
    "            claim_groups = [claim_gr for claim_gr in item.get_truthy_claim_groups().values()]\n",
    "            print(f\"Number of claim groups extracted: {len(claim_groups)}\")\n",
    "            for claim_gr in claim_groups: \n",
    "#             set a count here to only consider first ten claims of a claim group \n",
    "                \n",
    "                for claim in claim_gr: \n",
    "                    if return_urls: \n",
    "                        ref_url_list = [get_ref_url(elem) for elem in get_reference_elements(claim) if elem]\n",
    "                        statement = \"\"\n",
    "                        try:\n",
    "                            if claim.mainsnak:\n",
    "                                property_value, value = get_claim_values(claim)\n",
    "                                statement = \" \".join([str(item.get_enwiki_title()), str(property_value), str(value)])\n",
    "                                \n",
    "                            ref_items.append({statement: ref_url_list})\n",
    "                                \n",
    "                        except Exception as e: \n",
    "                            print(f\"The following exception occurred while executing function 'get_reference_items_titles_given': {e}\")\n",
    "                            ref_items.append({statement: ref_url_list})\n",
    "\n",
    "#                         ref_items.append({statement: ref_url_list})\n",
    "#                         ref_items.extend([get_ref_url(elem) for elem in get_reference_elements(claim) if elem])\n",
    "                    else:\n",
    "                        ref_items.extend(get_reference_elements(claim))\n",
    "    \n",
    "    except Exception as e: \n",
    "        print(f\"The following exception occurred while executing function 'get_reference_items_titles_given': {e}\")\n",
    "\n",
    "    return ref_items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_url(ref) -> str: \n",
    "    \n",
    "    if type(ref) == str: \n",
    "        # ref is already a URL\n",
    "        return ref\n",
    "    \n",
    "    url = [snak.datavalue.value for pid, snaks in ref.snaks.items() for snak in snaks \n",
    "           if snak.property_id == 'P854']\n",
    "    if url and len(url)>0: \n",
    "        return url[0]\n",
    "    else:\n",
    "        return None \n",
    "\n",
    "\n",
    "def has_formatter_url(pid: str): \n",
    "    \"\"\"\n",
    "    Given pid of a property find out if it has a P1630 (=formatter_URL) property.\n",
    "    If so return True, else False. \n",
    "    \"\"\"\n",
    "    prop_dict = get_entity_dict_from_api(pid)\n",
    "    has_form_url = False\n",
    "\n",
    "    if 'P1630' in prop_dict['claims']:\n",
    "        has_form_url = True\n",
    "        \n",
    "    return has_form_url\n",
    "\n",
    "\n",
    "# Information on Wikidata elemens e.g. snaks, main snaks, etc. \n",
    "# https://qwikidata.readthedocs.io/en/stable/entity.html#examples\n",
    "\n",
    "def get_reference_elements(claim: qwikidata.claim.WikidataClaim) ->list: \n",
    "    \"\"\"\n",
    "    This function checks if input claim has URL references and returns them \n",
    "    \n",
    "    Parameters:\n",
    "    claim(): claim for which references should be retrived\n",
    "    \n",
    "    Returns: \n",
    "    list: list of references which reference an external URL \n",
    "    \"\"\"\n",
    "    \n",
    "    ref_list = []\n",
    "    try:\n",
    "        for ref_num, ref in enumerate(claim.references):\n",
    "            for pid, snaks in ref.snaks.items():\n",
    "                \n",
    "                if pid == \"P854\": # \"reference URL\" property\n",
    "                    for snak in snaks:\n",
    "                        ref_list.append(snak.datavalue.value)  \n",
    "\n",
    "                elif pid == \"P248\": # \"stated in\" property\n",
    "                    property_id = claim.property_id    \n",
    "                    prop_dict = get_entity_dict_from_api(property_id)\n",
    "                    \n",
    "                    if 'P1630' in prop_dict['claims']: # \"formatter URL\" property\n",
    "                        formatter_url = prop_dict['claims']['P1630'][0]['mainsnak']['datavalue']['value']\n",
    "                        entry_id = claim.mainsnak.datavalue.value\n",
    "                        url = formatter_url.replace('$1', str(entry_id))\n",
    "                        ref_list.append(url)\n",
    "                        \n",
    "                    elif 'P2888' in prop_dict['claims']: # exact match \n",
    "                        exact_url = prop_dict['claims']['P2888'][0]['mainsnak']['datavalue']['value']\n",
    "                        ref_list.append(exact_url)\n",
    "                        \n",
    "                elif has_formatter_url(pid):\n",
    "                    # check if one of the other reference properties include P1630\n",
    "                    \n",
    "                    prop_dict = get_entity_dict_from_api(pid)\n",
    "                    formatter_url = prop_dict['claims']['P1630'][0]['mainsnak']['datavalue']['value']                  \n",
    "                    entry_id = snaks[0].datavalue.value\n",
    "                    formatted_url = formatter_url.replace('$1', str(entry_id))\n",
    "                    ref_list.append(formatted_url)\n",
    "\n",
    "    except Exception as e: \n",
    "        print(f\"The following error occurred in function 'get_reference_elements': {e}\")\n",
    "        \n",
    "    ref_list = list(set(ref_list))\n",
    "    \n",
    "    return ref_list\n",
    "\n",
    "\n",
    "def get_claim_values(claim: qwikidata.claim.WikidataClaim): \n",
    "    \"\"\"\n",
    "    Gives back the property name and referenced value of a Wikidata claim\n",
    "    \n",
    "    Parameters: \n",
    "    claim (WikidataClaim): claim for which the values should be retrieved\n",
    "    \n",
    "    Returns: \n",
    "    (property_value, value): a set of two values, one for the property used in the claim and the secondly the referenced value\n",
    "    \"\"\"\n",
    "    main = claim.mainsnak\n",
    "    value = main.datavalue.value if main.datavalue else \" \"\n",
    "    \n",
    "    if type(value) == dict and \"id\" in value:\n",
    "        qid = value[\"id\"]\n",
    "        value = get_wikidata_item(qid).get_enwiki_title()\n",
    "\n",
    "    property_dict = get_entity_dict_from_api(main.property_id) if main.property_id else \" \"\n",
    "\n",
    "    if property_dict and property_dict!=\" \" and property_dict[\"labels\"] and property_dict[\"labels\"][\"en\"] and property_dict[\"labels\"][\"en\"][\"value\"]:\n",
    "        property_value = property_dict[\"labels\"][\"en\"][\"value\"]\n",
    "    else: \n",
    "        property_value = \"\"\n",
    "        \n",
    "    return (property_value, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5308c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiprocessing_get_wikipedia_externallinks(df: pd.DataFrame, num_processes: int, col_name = \"wiki_entities\"):\n",
    "    \"\"\"\n",
    "    Multiprocessing of function get_wikipedia_externallinks\n",
    "    \"\"\"\n",
    "    pool = multiprocessing.Pool(processes = 10)\n",
    "    result = pool.map(get_wikipedia_externallinks, zip(df[col_name], df.index.values))\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79470243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_externallinks(input_data) -> list: \n",
    "    \"\"\"\n",
    "    Returns for a given Wikipedia page title, the list of external links (websites)\n",
    "    \"\"\"\n",
    "    titles, index = input_data\n",
    "    print(f\"Function get_wikipedia_externallinks executed for index {index}.\")\n",
    "    references = []\n",
    "    \n",
    "    for wiki_title in titles:\n",
    "        try:\n",
    "            if len(list(set(references)))>800: \n",
    "                break\n",
    "            page = wikipedia.page(wiki_title)\n",
    "            references.extend(page.references)\n",
    "\n",
    "        except Exception as e: \n",
    "            logger.info(f\"Following exception occurred while retrieving website text: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # remove duplicate urls\n",
    "    references = list(set(references))\n",
    "    print(f\"{len(references)} URLs have been retrieved for index {index}.\")\n",
    "    \n",
    "    return references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_docs(claim: str, documents: list, method: str, top_k: int):\n",
    "    \"\"\"\n",
    "    Reranks the list of documents according to their relevance given the claim text.\n",
    "    Applicable are one of two methods: tfidf, bm25\n",
    "    \"\"\"\n",
    "    print(claim)\n",
    "    print(len(documents))\n",
    "    print(method)\n",
    "    print(top_k)\n",
    "    \n",
    "    if method == \"tfidf\": \n",
    "        logger.info(f\"Reranking websites using Tfidf\")\n",
    "        \n",
    "        # Get embeddings for claim and website's text\n",
    "        vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer, ngram_range=(2,3))\n",
    "        doc_vectors = vectorizer.fit_transform([claim] + documents)\n",
    "\n",
    "        # calculate similarity \n",
    "        cosine_similarities = linear_kernel(doc_vectors[0:1], doc_vectors).flatten()\n",
    "        document_scores = [item.item() for item in cosine_similarities[1:]]\n",
    "        \n",
    "        # sort entries based on relevance for claim\n",
    "        sorted_entries = dict(zip(documents, document_scores))\n",
    "        sorted_entries = dict(sorted(sorted_entries.items(), key=lambda item: item[1], reverse = True))\n",
    "        reranked_docs = list(sorted_entries.keys()) if sorted_entries.keys() else []\n",
    "\n",
    "    elif method == \"bm25\":\n",
    "        # Retrieve and rerank with BM25 (from BIER benchmark paper inspired: https://arxiv.org/abs/2104.08663)\n",
    "        logger.info(f\"Reranking websites using BM25\")\n",
    "        reranked_docs = retrieve_and_rank(claim, documents, top_k = top_k)\n",
    "        \n",
    "    else: \n",
    "        logger.info(\"No appropriate method selected for ranking documents. Unranked list will be returned.\")\n",
    "        reranked_docs = documents\n",
    "        \n",
    "    return reranked_docs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d6afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_wikimedia_references_multiprocessing(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Calls function 'get_top_wikimedia_references' using multiple threads \n",
    "    \"\"\" \n",
    "#     num_processes = multiprocessing.cpu_count()\n",
    "    num_processes = 10\n",
    "    pool = multiprocessing.Pool(processes = num_processes)\n",
    "    result = pool.map(get_top_wikimedia_references, zip(df[\"claim\"], df[\"urls_wikipedia\"], df.index.values))\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70518173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_wikimedia_references(input_data):\n",
    "    \"\"\"\n",
    "    Given a list of URLs retrieved from either wikipedia or wikidata and a sentence (e.g. a claim),\n",
    "    sort URLs based on their similarity to the sentence and return top k websites \n",
    "    \"\"\"\n",
    "    claim, page_urls_wikipedia, index = input_data\n",
    "    top_k = 150 # number of pages to return after sorting \n",
    "    \n",
    "    page_urls = page_urls_wikipedia\n",
    "    logger.info(f\"Function 'get_top_wikimedia_references' being executed for index: {index}\")\n",
    "    \n",
    "    time_out = 10 # timeout for http requests \n",
    "    blacklist = ['[document]', 'noscript', 'header', 'html', 'meta', 'head', 'input', 'script', 'style', 'footer'] # ignore texts in these tags\n",
    "\n",
    "    website_texts = []\n",
    "    websites_dict = {}\n",
    "    \n",
    "    with FuturesSession() as session:\n",
    "        futures = [session.get(page_url, timeout=time_out) for page_url in page_urls if \"web.archive.org\" not in page_url]                         \n",
    "\n",
    "        # Load websites and extract text\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                resp = future.result()\n",
    "                content_type = resp.headers['Content-Type'] if hasattr(resp,'headers') and 'Content-Type' in resp.headers else None\n",
    "                if content_type and 'text/html' in content_type:\n",
    "                    soup = BeautifulSoup(resp.content, 'html.parser', from_encoding=\"iso-8859-1\")\n",
    "\n",
    "                    # load here text as well \n",
    "                    text = soup.find_all(text=True)\n",
    "                    texts = \" \".join([str(t) for t in text if t.name not in blacklist and t.parent.name not in blacklist and str(t).strip() != \"\"])\n",
    "\n",
    "                    # cleaning website's main text\n",
    "                    texts = texts.replace('\\n', ' ')\n",
    "                    texts = texts.replace('</s>', '')\n",
    "                    \n",
    "                    texts = re.sub(r\"\\\\\\w+|w+\\\\w+\", \"\", texts)                \n",
    "                    texts = texts.strip()\n",
    "\n",
    "                    website_texts.append(\"\".join(texts))\n",
    "                    websites_dict[texts] = resp.url \n",
    "                    \n",
    "            except (TooManyRedirects) as e:\n",
    "                resp = e.response\n",
    "                error_msg = str(e)\n",
    "                logging.info(f\"The following error occured while retrieving a website: TooManyRedirects\")\n",
    "                continue\n",
    "\n",
    "            except (ReadTimeout, ConnectTimeout) as e:\n",
    "                resp = requests.models.Response()\n",
    "                resp.reason = 'timeout'\n",
    "                resp.status_code = 408\n",
    "                error_msg = str(e)\n",
    "                logging.info(f\"The following error occured while retrieving a website: ReadTimeout, ConnectTimeout - {error_msg}\")\n",
    "                continue\n",
    "\n",
    "            except (ConnectionError) as e:\n",
    "                #print(r)\n",
    "                #traceback.print_exc()\n",
    "                resp = requests.models.Response()\n",
    "                resp.reason = 'connection error'\n",
    "                resp.status_code = 500\n",
    "                error_msg = str(e)\n",
    "                logging.info(f\"The following error occured while retrieving a website: ConnectionError - {error_msg}\")\n",
    "                continue\n",
    "                \n",
    "            except InvalidSchema as e:\n",
    "                try:\n",
    "                    resp = requests.models.Response()\n",
    "                    with urllib.request.urlopen(r, timeout=timeout[1]) as resp_urllib:\n",
    "                        resp.headers = resp_urllib.headers\n",
    "                        resp.url = resp_urllib.url\n",
    "                        error_msg = str(e)\n",
    "                        resp.status_code = resp_urllib.code if hasattr(resp_urllib,'code') else None\n",
    "                        resp.reason = resp_urllib.reason if hasattr(resp_urllib,'reason') else None\n",
    "                except urllib.error.URLError as e:\n",
    "                    resp = requests.models.Response()\n",
    "                    resp.reason = 'malformed or invalid url'\n",
    "                    resp.status_code = 400\n",
    "                    error_msg = str(e)\n",
    "                    logging.info(f\"The following error occured while retrieving a website: malformed or invalid url\")\n",
    "                    continue\n",
    "                except Exception as e: \n",
    "                    logging.info(f\"The following error occured while retrieving a website: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "            except (UnicodeError, InvalidURL, ValueError) as e:\n",
    "                resp = requests.models.Response()\n",
    "                resp.reason = 'malformed or invalid url'\n",
    "                resp.status_code = 400\n",
    "                error_msg = str(e)\n",
    "                logging.info(f\"The following error occured while retrieving a website: UnicodeError, InvalidURL\")\n",
    "                continue\n",
    "                \n",
    "            except Exception as e: \n",
    "                logging.info(f\"The following error occured while retrieving a website: {e}\")\n",
    "                continue\n",
    "                \n",
    "        \n",
    "        # Rerank websites based on similarity between website's text and claim\n",
    "        if not websites_dict: \n",
    "            return []\n",
    "        \n",
    "        ranked_docs = rerank_docs(claim, list(websites_dict.keys()), 'bm25', top_k)\n",
    "        result_list = [websites_dict[doc] for doc in ranked_docs][:top_k]\n",
    "        \n",
    "        print(f\"lengths of result list is: {len(result_list)}\")\n",
    "\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d1ec2e",
   "metadata": {},
   "source": [
    "###  <strong>Get top Tables from extracted Evidence Websites & rank them</strong> \n",
    "\n",
    "* Extract tables from top websites\n",
    "* Rank tables based on similarity to claim text\n",
    "\n",
    "\n",
    "=> with notebook Webtables_extraction_from_HTML.ipynb\n",
    "\n",
    "\n",
    "* Analyse tables to get an understanding of the tables there\n",
    "* Some further preprocessing steps needed to filter out junk tables?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8b2c1",
   "metadata": {},
   "source": [
    "#### 1.) Extract tables from top websites (from Wikidata and -pedia references) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3, socket\n",
    "from urllib3.connection import HTTPConnection\n",
    "\n",
    "HTTPConnection.default_socket_options = ( \n",
    "    HTTPConnection.default_socket_options + [\n",
    "    (socket.SOL_SOCKET, socket.SO_SNDBUF, 1000000), #1MB in byte\n",
    "    (socket.SOL_SOCKET, socket.SO_RCVBUF, 1000000)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc577c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    return convert_tables_multiprocess(train_data, column_name = \"sorted_wiki_references\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": \n",
    "    result = main()\n",
    "#     test_data[\"tables_wiki_references\"] = [[t for t in entry if t!=[]] if entry and any(entry) else [] for entry in result]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a4c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_data_tables_wiki_references.pkl\", \"wb\") as file: \n",
    "    pickle.dump(result, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# path = f\"train_data_tables_wiki_references_{begin_index}till{end_index}.pkl\"\n",
    "path = f\"train_data_tables_wiki_references_{begin_index}till_end.pkl\"\n",
    "\n",
    "with open(path, \"rb\") as file: \n",
    "    x = pickle.load(file)\n",
    "#     for index, row in train_data[begin_index:end_index].iterrows(): \n",
    "    for index, row in train_data[begin_index:].iterrows(): \n",
    "        train_data.at[index, \"tables_wiki_references\"] = x[index-begin_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a57fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"tables_wiki_references\"] = \"\"\n",
    "\n",
    "for index, row in train_data.iterrows(): \n",
    "    train_data.at[index, \"tables_wiki_references\"] = result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa31ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of entities \n",
    "entries_len = [len(entries) for entries in train_data[\"tables_wiki_references\"] if entries and type(entries)==list]\n",
    "df = pd.DataFrame(entries_len)\n",
    "\n",
    "print(df.describe())\n",
    "\n",
    "ax = sns.boxplot(x=entries_len)\n",
    "ax.set_title(\"Number of tables extracted for each claim\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6450f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import * \n",
    "\n",
    "def rerank_tables(claim: str, tables: list, top_k: int):\n",
    "    \"\"\"\n",
    "    Reranks the list of documents according to their relevance given the claim text.\n",
    "    Applicable are one of two methods: tfidf, bm25\n",
    "    \"\"\"\n",
    "    assert isinstance(tables, list)\n",
    "    \n",
    "    logging.info(f\"Reranking websites using BM25\")\n",
    "    try: \n",
    "        table_entries = dict()\n",
    "        for t in tables: \n",
    "            if len(t[\"rows_list\"]) > 15: # if more than X rows, skip\n",
    "                continue\n",
    "            elif (t[\"header_horizontal\"] and len(t[\"header_horizontal\"])>10):\n",
    "                continue\n",
    "            elif (t[\"header_vertical\"] and len(t[\"header_vertical\"])>10):\n",
    "                continue\n",
    "                \n",
    "            cell_len = [len(cell) for row in t[\"rows_list\"] for cell in row] # if one cell has more than 1k characters, skip\n",
    "            if cell_len and max(cell_len) > 250:\n",
    "                continue\n",
    "\n",
    "            col_len = [len(row) for row in t[\"rows_list\"]] # if ten or more columns, skip \n",
    "            if col_len and max(col_len) > 15:\n",
    "                continue\n",
    "\n",
    "            if t[\"header_horizontal\"]: # if one header cell has more than 100 characters, skip\n",
    "                header_h_len = [len(cell) for cell in t[\"header_horizontal\"]]\n",
    "                if max(header_h_len) > 100:\n",
    "                    continue\n",
    "\n",
    "            if t[\"header_vertical\"]: # if one header cell has more than 1k characters, skip\n",
    "                header_v_len = [len(cell) for cell in t[\"header_vertical\"]]\n",
    "                if max(header_v_len) > 100:\n",
    "                    continue\n",
    "\n",
    "            comparison_txt = get_table_caption(t)\n",
    "\n",
    "            if not comparison_txt.strip() or len(comparison_txt)<10: \n",
    "                comparison_txt = get_table_header(t)\n",
    "                if not comparison_txt.strip() or len(comparison_txt)<10: \n",
    "                    comparison_txt = get_table_text(t)\n",
    "            table_entries[comparison_txt] = str(t[\"id\"])\n",
    "\n",
    "        documents = list(table_entries.keys())\n",
    "        \n",
    "        # TFIDF\n",
    "        # Get embeddings for claim and website's text\n",
    "#         vectorizer = TfidfVectorizer(stop_words=token_stop, tokenizer=tokenizer, ngram_range=(2,3))\n",
    "#         doc_vectors = vectorizer.fit_transform([claim] + documents)\n",
    "\n",
    "        # calculate similarity \n",
    "#         cosine_similarities = linear_kernel(doc_vectors[0:1], doc_vectors).flatten()\n",
    "#         document_scores = [item.item() for item in cosine_similarities[1:]]\n",
    "\n",
    "#         # sort entries based on relevance for claim\n",
    "#         sorted_entries = dict(zip(documents, document_scores))\n",
    "#         sorted_entries = dict(sorted(sorted_entries.items(), key=lambda item: item[1], reverse = True))\n",
    "#         reranked_docs = list(sorted_entries.keys()) if sorted_entries.keys() else []\n",
    "\n",
    "#         reranked_tables = []\n",
    "#         for doc in reranked_docs: \n",
    "#             _id = table_entries[doc]\n",
    "#             reranked_tables.append([entry for entry in tables if str(entry[\"id\"])==_id][0])\n",
    "\n",
    "#         print(len(reranked_tables))\n",
    "#         top_doc = reranked_tables[:top_k]\n",
    "\n",
    "        # BM25\n",
    "        tokenized_docs = [doc.split(\" \") for doc in docum,ents]\n",
    "        bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "        tokenized_query = claim.split(\" \")\n",
    "        top_doc = bm25.get_top_n(tokenized_query, docs, n=top_k)\n",
    "\n",
    "        return top_doc\n",
    "    \n",
    "    except Exception as e: \n",
    "        logging.info(f\"Following error occured whiled ranking tables: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_result = []\n",
    "\n",
    "for index, row in train_data.iterrows(): \n",
    "    print(f\"Index of processed row: {index}\")\n",
    "    table_result.append(rerank_tables(row[\"claim\"], row[\"tables_wiki_references\"], 15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update MongoDB \n",
    "\n",
    "for index, row in train_data[400:].iterrows():\n",
    "    print('Updating index',index,end='\\r')\n",
    "    try:\n",
    "        train_col.update_one({'_id': row[\"_id\"]},\n",
    "                             {'$set': {'sorted_tables_wikipedia': table_result[index-400]}})\n",
    "    except Exception as e: \n",
    "        print(index)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of entities \n",
    "entries_len = [len(entries) for entries in table_result if entries and type(entries)==list]\n",
    "df = pd.DataFrame(entries_len)\n",
    "\n",
    "print(df.describe())\n",
    "\n",
    "ax = sns.boxplot(x=entries_len)\n",
    "ax.set_title(\"Number of tables extracted for each claim\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9de719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cell_value(cell_val: str) -> str:\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "    cell_val (str): string to be cleaned\n",
    "\n",
    "    Returns: \n",
    "    str: cleaned value\n",
    "    \"\"\"\n",
    "    val = unicodedata.normalize('NFKD', cell_val)\n",
    "    val = val.encode('ascii', errors='ignore')\n",
    "    val = str(val, encoding='ascii')\n",
    "    \n",
    "    val = clean_wiki_template(val)\n",
    "    val = re.sub(r'\\s+', ' ', val).strip()\n",
    "\n",
    "    return val\n",
    "\n",
    "def clean_wiki_template(text):\n",
    "    if re.match(r'^{{.*}}$', text):\n",
    "        text = text[2:-2].split('|')[-1]  # remove {{ }}, and retain the last\n",
    "    else:\n",
    "        text = re.sub(r'{{.*}}', '', text)\n",
    "        \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f166e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]', 'table', 'th', 'td', 'footer']:\n",
    "        return False\n",
    "    if isinstance(element, bs4.element.Comment):\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tables_multiprocess(df: pd.DataFrame, column_name: str):\n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "#     num_processes = 10\n",
    "    logger.info(f\"Number of processes: {num_processes}\")\n",
    "\n",
    "    try:\n",
    "        pool = multiprocessing.Pool(processes=num_processes)\n",
    "        result = pool.map(convert_tables_parallel, zip(df[column_name], df[\"claim\"], df[\"wiki_entities\"], \n",
    "                                                       df.index.values))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "                \n",
    "    except Exception as e: \n",
    "        logger.info(f\"The following exception occurred while function 'convert_tables_multiprocess': {e}\")\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65636a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tables_parallel(input_data) -> list:\n",
    "    \"\"\"\n",
    "    iterates over a list of urls and returns their tables in a list.\n",
    "    Additional settings: time_out\n",
    "\n",
    "    Parameters:\n",
    "    page_url (list): list of source urls \n",
    "    index: an int showing the index of the row processed \n",
    "\n",
    "    Returns: \n",
    "    list: list containing all tables found on source url pages \n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page_urls, claim, wiki_entities, index = input_data\n",
    "        logging.info(f\"Index of processed row: {index}\")\n",
    "        \n",
    "        if not page_urls or page_urls == [] or type(page_urls) != list:\n",
    "            return [] # if URL list is empty \n",
    "        \n",
    "        keywords = [w.lower() for wiki_entity in wiki_entities for w in tokenizer(wiki_entity) if w not in token_stop]\n",
    "        keywords.extend([w.lower() for w in tokenizer(claim) if w not in token_stop])\n",
    "        keywords = list(set(keywords)) # create list of keywords the website will be compared against\n",
    "        conv_tables = []\n",
    "        time_out = 10 # adjust if longer timeout nedded\n",
    "        \n",
    "        # Iterate over list of urls in parallel \n",
    "        with FuturesSession() as session:\n",
    "            futures = [session.get(page_url, timeout=time_out) for page_url in page_urls if \"web.archive.org\" not in page_url] \n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    page = future.result(timeout=3)  \n",
    "                    content_type = page.headers['Content-Type'] if hasattr(page,'headers') and 'Content-Type' in page.headers else None\n",
    "                    \n",
    "                    # Check header before further processing\n",
    "                    if not content_type or 'text/html' not in content_type:\n",
    "                        # skip this website \n",
    "                        logger.info(\"Website skipped because content_type not text/html.\")\n",
    "                        continue \n",
    "                        \n",
    "                    soup = BeautifulSoup(page.content, 'html.parser', from_encoding=\"iso-8859-1\")\n",
    "\n",
    "                    # check if English website, otherwise skip website  \n",
    "                    texts = soup.findAll(text=True)\n",
    "                    visible_texts = filter(tag_visible, texts)  \n",
    "                    text = u\" \".join(t.strip() for t in visible_texts)\n",
    "                    if text.strip()!=\"\" and detect(text) and detect(text) != \"en\":\n",
    "                        logger.info(\"Given URL is a non-English website.\")\n",
    "                        continue\n",
    "                        \n",
    "                    keyword_match = [w for w in text.split(\" \") if w.lower() in keywords]\n",
    "                    if len(keyword_match)<3: \n",
    "                        # if less than three keywords occur in website's texts => skip\n",
    "                        continue\n",
    "\n",
    "                    # Extract all HTML tables \n",
    "                    tables = soup.find_all('table') \n",
    "                    if not tables or tables == []: # no table on website => skip\n",
    "                        continue\n",
    "\n",
    "                    # Adding further textual sources from website: title and meta data if available \n",
    "                    title = soup.find('head').find('title').text if soup.find('head') and soup.find('head').find('title') and soup.find('head').find('title').text else \"\" \n",
    "                    title = clean_cell_value(title.strip())\n",
    "\n",
    "                    meta_data = soup.find('meta', {\"name\": \"description\"})\n",
    "                    meta_data = meta_data[\"content\"] if meta_data and \"content\" in meta_data else \"\"\n",
    "\n",
    "                    meta_data_keywords = soup.find('meta', {\"name\": \"keywords\"})\n",
    "                    meta_data_keywords = meta_data_keywords[\"content\"] if meta_data_keywords and \"content\" in meta_data_keywords else \"\"\n",
    "\n",
    "                    meta_data += meta_data_keywords\n",
    "                    meta_data = clean_cell_value(meta_data.strip())\n",
    "\n",
    "#                     html_tables[page.url] = html_tables_to_dict(tables, title, meta_data)\n",
    "                    table_dict_list = html_tables_to_dict(tables = tables, title = title, meta_data = meta_data, \n",
    "                                                          page_url = page.url) # gives back a list of table dictionaries \n",
    "    \n",
    "                    if table_dict_list not in conv_tables:\n",
    "                        conv_tables.extend(table_dict_list)\n",
    "                \n",
    "                \n",
    "                except (TooManyRedirects) as e:\n",
    "#                     resp = e.response\n",
    "                    error_msg = str(e)\n",
    "                    logging.info(f\"The following error occured while retrieving a website: TooManyRedirects\")\n",
    "                    continue\n",
    "\n",
    "                except (ReadTimeout, ConnectTimeout) as e:\n",
    "#                     resp = requests.models.Response()\n",
    "#                     resp.reason = 'timeout'\n",
    "#                     resp.status_code = 408\n",
    "                    error_msg = str(e)\n",
    "                    logging.info(f\"The following error occured while retrieving a website: ReadTimeout, ConnectTimeout - {error_msg}\")\n",
    "                    continue\n",
    "\n",
    "                except (ConnectionError) as e:\n",
    "                    #print(r)\n",
    "                    #traceback.print_exc()\n",
    "#                     resp = requests.models.Response()\n",
    "#                     resp.reason = 'connection error'\n",
    "#                     resp.status_code = 500\n",
    "                    error_msg = str(e)\n",
    "                    logging.info(f\"The following error occured while retrieving a website (Connection error) - {error_msg}\")\n",
    "                    continue\n",
    "\n",
    "                except (UnicodeError, InvalidURL, ValueError) as e:\n",
    "#                     resp = requests.models.Response()\n",
    "#                     resp.reason = 'malformed or invalid url'\n",
    "#                     resp.status_code = 400\n",
    "                    error_msg = str(e)\n",
    "                    logging.info(f\"The following error occured while retrieving a website: UnicodeError, InvalidURL\")\n",
    "                    continue\n",
    "                \n",
    "                except Exception as e: \n",
    "                    logger.info(f\"The following exception occurred while executing 'convert_tables_parallel': {e}\")\n",
    "                    continue        \n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.info(f\"The following exception occurred while starting function 'convert_tables_parallel': {e}\")\n",
    "        pass\n",
    "    \n",
    "    return conv_tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_tables_multiprocess(df: pd.DataFrame):\n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "#     logging.info(f\"Number of processes: {num_processes}\")\n",
    "    print(f\"Number of processes: {num_processes}\")\n",
    "\n",
    "    try:\n",
    "        pool = multiprocessing.Pool(processes=5)\n",
    "        result = pool.map(rerank_tables_parallel, zip(df[\"claim\"], df[\"tables_wiki_references\"]))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"The following exception occurred while function 'rerank_tables_multiprocess': {e}\")\n",
    "#         logging.info(f\"The following exception occurred while function 'rerank_tables_multiprocess': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd73ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_tables_to_dict(tables: list, title = \"\", meta_data = \"\", page_url = \"\", w_statement = \"\", include_small_tables = False) -> list:\n",
    "    \"\"\"\n",
    "    Transforms html table into a dict object and filters out small tables if 'include_small_tables' is set False.\n",
    "    Returns list of table dictionaries of following format: \n",
    "     \n",
    "     tab_dict = {\n",
    "            '_id': uuid, \n",
    "            'title': title,\n",
    "            'meta_data': meta_data,\n",
    "            'caption': caption,\n",
    "            'wikidata_statement': w_statement,\n",
    "            'header_horizontal': header_horizontal,\n",
    "            'header_vertical': header_vertical,\n",
    "            'rows_list': rows,\n",
    "            'html_table'\n",
    "        }\n",
    "    \n",
    "    Parameters: \n",
    "    tables (list): list of bs4.element.table elements to convert \n",
    "    title (str, optional): title extracted from the website\n",
    "    meta_data (str, optional): meta data (e.g. description) extracted from the website\n",
    "    w_statement (str, optional): Wikidata statement of the reference the tables have been extracted from \n",
    "    include_small_tables (bool, optional): whether to include small tables e.g. <3 rows and <2 columns or not \n",
    "    \n",
    "    Returns:\n",
    "    list: a list of dictionaries, each dictionary equals one converted table\n",
    "    \"\"\"\n",
    "    tables_dict_list = []\n",
    "\n",
    "    for table in tables:\n",
    "        if not isinstance(table, bs4.element.Tag):\n",
    "            # skip if not table instance or table has inner-tables\n",
    "            continue\n",
    "        \n",
    "        if table.caption is None: \n",
    "            caption = None\n",
    "        else: \n",
    "            caption = table.caption.text\n",
    "            caption = re.sub(r'(\\[\\d+\\])+$', '', clean_cell_value(caption)) ###### TODO check function clean_cell_value ######\n",
    "\n",
    "        header_horizontal = []\n",
    "        header_vertical = []\n",
    "        if table.find('thead'):\n",
    "            header_horizontal = [(clean_cell_value(th.text)) for th in table.find('thead').find_all(['th', 'td'])]\n",
    "\n",
    "        rows = []\n",
    "        count = 0\n",
    "        if table.find('tbody'):\n",
    "            tr_cells_list = [tr for tr in table.find('tbody').find_all('tr')]\n",
    "        else:\n",
    "            tr_cells_list = [tr for tr in table.find_all('tr')]\n",
    "\n",
    "        for tr in tr_cells_list:\n",
    "\n",
    "            if not header_horizontal and count == 0: # 'tr' is the first row of the table and no 'thead' is given\n",
    "                count = 1\n",
    "                header_horizontal = [(clean_cell_value(th.text)) for th in tr.find_all(['th'])]\n",
    "\n",
    "                if not header_horizontal and tr.find('b'): # set first row as header if written in bold \n",
    "                    header_horizontal = [(clean_cell_value(th.text)) for th in tr.find_all(['td'])]\n",
    "                else:\n",
    "                    row = [(clean_cell_value(td.text)) for td in tr.find_all('td') if clean_cell_value(td.text).strip()!=\"\"]\n",
    "                    if row:\n",
    "                        rows.append(row)\n",
    "\n",
    "            else: # either 'tr' is not first row OR 'tr' is first row but a header is already given in 'thead'\n",
    "                count = 1 \n",
    "                row = []\n",
    "                \n",
    "                # \n",
    "\n",
    "                if not header_vertical: # adjust must not always be be header_vertical \n",
    "                    header_vertical = [(clean_cell_value(th.text)) for th in tr.find_all('th')]\n",
    "                else: \n",
    "                    for th in tr.find_all('th'):\n",
    "                        header_vertical.append(clean_cell_value(th.text))\n",
    "                row = [(clean_cell_value(td.text)) for td in tr.find_all('td') if clean_cell_value(td.text).strip()!=\"\"]\n",
    "                \n",
    "                if row:\n",
    "                    rows.append(row)\n",
    "\n",
    "        tab_dict = {\n",
    "            'id': uuid.uuid1(),\n",
    "            'url': page_url, # website from where tables have been extracted  \n",
    "            'title': title,\n",
    "            'meta_data': meta_data,\n",
    "            'caption': caption,\n",
    "            'wikidata_statement': w_statement,\n",
    "            'header_horizontal': header_horizontal,\n",
    "            'header_vertical': header_vertical,\n",
    "            'rows_list': rows,\n",
    "            'html_table': str(table)\n",
    "        }\n",
    "        \n",
    "        # Filter for tables which have less than three rows \n",
    "        if len(rows)<3 and not include_small_tables:\n",
    "            # table too small to be added \n",
    "            continue\n",
    "            \n",
    "        rows_len = [len(row) for row in rows]\n",
    "        avg_col_length = sum(rows_len)/len(rows_len)\n",
    "        \n",
    "        # Filter for tables which have less than 1.5 columns in average \n",
    "        if round(avg_col_length,1)<1.5 and not include_small_tables:\n",
    "            # table too small to be added \n",
    "            continue \n",
    "    \n",
    "        tables_dict_list.append(tab_dict)\n",
    "\n",
    "    return tables_dict_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
